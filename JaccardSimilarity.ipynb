{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180c04f7-431f-4506-8169-71f0942cd728",
   "metadata": {},
   "source": [
    "# CSGY-6513 Big Data Final Project\n",
    "In this notebook, I implement the Jaccard Similarity model on Kaggle Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda85efe-ec90-4e03-a1a9-5e43f395a366",
   "metadata": {},
   "source": [
    "## 1. Setting Up PySpark, Read the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aaed9da-73e5-42e9-8a12-1a79597ce600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 19:19:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bigdata-fall22/lib/python3.7/site-packages/pyspark/sql/context.py:159: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "conf.set('spark.driver.memory', '4g')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99baad2-0512-44be-ae6f-669cba6fb15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "news_data = 'Combined_News_DJIA.csv'\n",
    "data_df = spark.read.format('csv').option('inferSchema','true').option('header','true').load(news_data)\n",
    "data_df = data_df.withColumn('Date', to_date('Date'))\n",
    "label_df = data_df.select(col('Date'), col('Label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cde47-f24f-4ed1-8f43-430465a5c2a2",
   "metadata": {},
   "source": [
    "## 2. Preprocessing, Encoding Texts into Binary Vector Representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5c257b-0fbc-474c-9da5-209507163a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the news headlines into column \"News\" for each day.\n",
    "data_df = data_df.withColumn('News', col('Top1'))\n",
    "for i in range(1, 26):\n",
    "    data_df = data_df.withColumn('News', concat_ws(' ', 'News', 'Top'+str(i)))\n",
    "\n",
    "# Lowercase them, remove letters that is not numbers and alphabets, and split them into words.\n",
    "data_df = data_df.withColumn('News', regexp_replace(lower(col('News')), r\"[^0-9a-z]\", \" \"))\n",
    "data_df = data_df.withColumn('News', split(col('News'), \" \"))\n",
    "\n",
    "# Discard unnecessary columns and prepare for the word count using \"explode\" function.\n",
    "data_df = data_df.select(col('Date'), col('Label'), col('News'))\n",
    "data_df = data_df.withColumn('News', explode(col('News'))).withColumnRenamed('News', 'word')\n",
    "data_df = data_df.withColumn('word', trim(col('word')))\n",
    "# Remove the row with empty string and \"b\". All the texts start with a letter \"b\" which is nothing to do with the news headlines.\n",
    "data_df = data_df.where((col('word')!='') & (col('word')!='b'))\n",
    "\n",
    "# Split the data into training and test sets.\n",
    "train_df = data_df.where(col('Date') < '2015-09-17')\n",
    "test_df = data_df.where(col('Date') >= '2015-09-17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a24fc2-e7aa-4c57-98ee-51073cdb73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick only the frequent words to form binary vectors.\n",
    "count_df = train_df.select(col('word')).where(col('Label')==1).groupBy('word').count()\n",
    "count_df = count_df.where(col('count')>=50).select(col('word'))\n",
    "\n",
    "# Instead of storing the actual binary vectors, we keep the index of entries with 1.\n",
    "# So, give index for each word.\n",
    "count_df = count_df.withColumn('index', monotonically_increasing_id()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e036957d-45e6-49b5-96da-810db0f4e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 19:19:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words Selected: 970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Words Selected: {count_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7dc15c-8715-4b02-ba77-300cf0ad169e",
   "metadata": {},
   "source": [
    "Here, our binary vectors have a length 970."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190cf0da-2cc9-42fc-ba92-b2461cd391d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the index to each word in our data.\n",
    "train_df = train_df.join(count_df, ['word'], 'left').na.drop()\n",
    "# Aggregate the indices for each day. \"collect_set\" function can do this.\n",
    "train_df = train_df.groupby('Date').agg(collect_set('index').alias('train'))\n",
    "train_df = train_df.withColumnRenamed('Date', 'trainDate')\n",
    "# Do the same thing for test set.\n",
    "test_df = test_df.join(count_df, ['word'], 'left').na.drop()\n",
    "test_df = test_df.groupby('Date').agg(collect_set('index').alias('test'))\n",
    "test_df = test_df.withColumnRenamed('Date', 'testDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286240e8-cde6-44a2-82bf-77c687645a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "| trainDate|               train|\n",
      "+----------+--------------------+\n",
      "|2008-08-08|[102, 814, 52, 96...|\n",
      "|2008-08-11|[466, 589, 110, 6...|\n",
      "|2008-08-12|[133, 968, 714, 8...|\n",
      "+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939357dd-5461-42de-bbc9-be8473d06b85",
   "metadata": {},
   "source": [
    "Now we have the binary vector representation. For example, [102, 814, 52, 96...] in the first row means, our binary vector is very sparse and has 1 at 102-nd, 814-th, 52-nd, 96-th, ... entries. Since each index is connected to a word, there is a word corresponding to the index 102 in our texts for 2008/08/08."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479028f2-e7c2-4f2f-9fed-5da2583933b0",
   "metadata": {},
   "source": [
    "## 3. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38f2274-997c-49a7-a749-72c2aa8190fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want to compute the Jaccard Similarity for all train-test pairs, we use \"crossJoin\".\n",
    "merge = train_df.crossJoin(test_df)\n",
    "# \"array_intersect\" and \"array_union\" are quite similar to set operations in Python.\n",
    "# The number of items in intersection divided by the number of items in union is exactly the Jaccard Similarity.\n",
    "merge = merge.withColumn('Jaccard', size(array_intersect(col('train'), col('test'))) / size(array_union(col('train'), col('test'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a68187a-ec50-463e-8663-133e9012ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Use the Window function and pick the one with highest Jaccard Similarity.\n",
    "windowSpec = Window.partitionBy('testDate').orderBy(desc('Jaccard'))\n",
    "merge = merge.withColumn('rank', rank().over(windowSpec)).where(col('rank')==1)\n",
    "# Concatenate the predicted label and ground truth.\n",
    "pred = merge.select(col('trainDate'), col('testDate')).join(label_df, merge['trainDate']==label_df['Date'], 'left')\n",
    "pred = pred.withColumnRenamed('Label', 'pred').drop('Date')\n",
    "pred = pred.join(label_df, merge['testDate']==label_df['Date'], 'left')\n",
    "pred = pred.withColumnRenamed('Label', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ed6856-bd74-484d-858c-f5bc5b607b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy.\n",
    "acc = pred.where(col('pred')==col('true')).count() / pred.count()\n",
    "print(f\"Test Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b88d51-199a-43e6-9cca-121bde33b87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-fall22]",
   "language": "python",
   "name": "conda-env-bigdata-fall22-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
