{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99067ad6-4fd3-44b6-a584-0b4629ca66aa",
   "metadata": {},
   "source": [
    "# CSGY-6513 Big Data Final Project\n",
    "In this notebook, I implement the Count-Based model on Kaggle Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04085d0a-7751-4ecb-8285-c33f876b23f1",
   "metadata": {},
   "source": [
    "## 1. Setting Up PySpark, Read the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d11b08-7806-4ead-927a-9bbdb337de33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 17:40:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bigdata-fall22/lib/python3.7/site-packages/pyspark/sql/context.py:159: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "conf.set('spark.driver.memory', '4g')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867cd178-7be4-4696-863d-e1f277a081dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "news_data = 'Combined_News_DJIA.csv'\n",
    "data_df = spark.read.format('csv').option('inferSchema','true').option('header','true').load(news_data)\n",
    "data_df = data_df.withColumn('Date', to_date('Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970a193f-6401-4476-846e-a642b677ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 17:42:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      Date|Label|                Top1|                Top2|                Top3|                Top4|                Top5|                Top6|                Top7|                Top8|                Top9|               Top10|               Top11|               Top12|               Top13|               Top14|               Top15|               Top16|               Top17|               Top18|               Top19|               Top20|               Top21|               Top22|               Top23|               Top24|               Top25|\n",
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|2008-08-08|    0|\"b\"\"Georgia 'down...|b'BREAKING: Musha...|b'Russia Today: C...|b'Russian tanks a...|\"b\"\"Afghan childr...|' U.N. official s...| a three year old...|b'150 Russian tan...|\"b\"\"Breaking: Geo...| Russia warned it...|\"b\"\"The 'enemy co...| but will be kept...|b'Georgian troops...|b'Did the U.S. Pr...|b'Rice Gives Gree...|b'Announcing:Clas...|\"b\"\"So---Russia a...|\"b\"\"China tells B...|b'Did World War I...|b'Georgia Invades...|b'Al-Qaeda Faces ...|\"b'Condoleezza Ri...|b'This is a busy ...|\"b\"\"Georgia will ...|000 soldiers from...|\n",
      "|2008-08-11|    1|b'Why wont Americ...|b'Bush puts foot ...|\"b\"\"Jewish Georgi...| we're fending of...|b'Georgian army f...|\"b\"\"Olympic openi...|b'What were the M...|b'Russia angered ...|b'An American cit...|b'Welcome To Worl...|  \"b\"\"Georgia's move| a mistake of mon...|b'Russia presses ...|b'Abhinav Bindra ...|b' U.S. ship head...|b'Drivers in a Je...|b'The French Team...|b'Israel and the ...|\"b'\"\"Do not belie...| neither Russian ...|b'Riots are still...|b'China to overta...|b'War in South Os...|b'Israeli Physici...|b' Russia has jus...|\n",
      "|2008-08-12|    0|b'Remember that a...|\"b\"\"Russia 'ends ...|\"b'\"\"If we had no...|\"b\"\"Al-Qa'eda is ...|b'Ceasefire in Ge...|b'Why Microsoft a...|b'Stratfor: The R...|\"b\"\"I'm Trying to...| Or Down If you T...|\"b\"\"The US milita...| a US defense off...|b'U.S. Beats War ...|\"b'Gorbachev: \"\"G...|b'CNN use footage...|b'Beginning a war...|b'55 pyramids as ...|b'The 11 Top Part...|b'U.S. troops sti...|b'Why Russias res...|\"b'Gorbachev accu...|b'Russia, Georgia...|b'Remember that a...|b'War in Georgia:...|b'All signs point...|b'Christopher Kin...|\n",
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed61db-1d06-40ff-b6cc-dc5a91e3e9d2",
   "metadata": {},
   "source": [
    "For each day, we have label and 25 news headlines. The label is 1 if the DJIA (Dow Jones Industrial Average) daily return is plus, and 0 if minus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13302aea-7143-408a-83b3-189436b0bf68",
   "metadata": {},
   "source": [
    "## 2. Preprocessing, Computing the Score for Each Frequent Word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26246a5f-d190-45f4-a8f2-8cb63635e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the news headlines into column \"News\" for each day.\n",
    "data_df = data_df.withColumn('News', col('Top1'))\n",
    "for i in range(1, 26):\n",
    "    data_df = data_df.withColumn('News', concat_ws(' ', 'News', 'Top'+str(i)))\n",
    "\n",
    "# Lowercase them, remove letters that is not numbers and alphabets, and split them into words.\n",
    "data_df = data_df.withColumn('News', regexp_replace(lower(col('News')), r\"[^0-9a-z]\", \" \"))\n",
    "data_df = data_df.withColumn('News', split(col('News'), \" \"))\n",
    "\n",
    "# Discard unnecessary columns and prepare for the word count using \"explode\" function.\n",
    "data_df = data_df.select(col('Date'), col('Label'), col('News'))\n",
    "data_df = data_df.withColumn('News', explode(col('News'))).withColumnRenamed('News', 'word')\n",
    "data_df = data_df.withColumn('word', trim(col('word')))\n",
    "# Remove the row with empty string and \"b\". All the texts start with a letter \"b\" which is nothing to do with the news headlines.\n",
    "data_df = data_df.where((col('word')!='') & (col('word')!='b'))\n",
    "\n",
    "# Split the data into training and test sets.\n",
    "train_df = data_df.where(col('Date') < '2015-09-17')\n",
    "test_df = data_df.where(col('Date') >= '2015-09-17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb88484-9f24-4ce6-8605-44bea0927432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform the word counting on training data.\n",
    "count_df = train_df.select(col('word')).where(col('Label')==1).groupBy('word').count()\n",
    "count_df = count_df.withColumnRenamed('count', 'pos')\n",
    "neg_df = train_df.select(col('word')).where(col('Label')==0).groupBy('word').count()\n",
    "neg_df = neg_df.withColumnRenamed('count', 'neg')\n",
    "count_df = count_df.join(neg_df, ['word'], 'outer')\n",
    "count_df = count_df.na.fill(value=0)\n",
    "count_df = count_df.withColumn('sum', col('pos')+col('neg'))\n",
    "\n",
    "# We only focus on frequent words, which is top 900 words in this case.\n",
    "count_df = count_df.where(col('sum')>100)\n",
    "print(f\"Number of Words: {count_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea3fcca5-d75e-4b56-8bde-1be8240d588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+-----+\n",
      "|word|               pos|               neg|  sum|\n",
      "+----+------------------+------------------+-----+\n",
      "| the| 5.506913833705694| 5.594660578965725|22443|\n",
      "|  to|4.5537941317181705| 4.541467178202649|18398|\n",
      "|  of|3.9295285959134763|3.8550154834754853|15755|\n",
      "|  in| 3.841741254940941| 3.846560343278692|15550|\n",
      "|   a|2.8036211116942957|2.8155616855321983|11364|\n",
      "| and|2.2360225367290156| 2.140735808575626| 8865|\n",
      "|   s| 1.684216393473081|1.6889142543094793| 6822|\n",
      "| for|1.6623856790513394|1.6434678757517147| 6689|\n",
      "|  on|1.3028756160209576|1.3501802001754442| 5360|\n",
      "|  is|1.1658530467781116|1.1583542069606942| 4702|\n",
      "+----+------------------+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Since the number of positives (the DJIA daily return is plus) and negatives are not the same, we normalize it.\n",
    "# Multiplying by 100 doesn't have a specific meaning. Just scaling so that the values don't get too small.\n",
    "pos_sum = count_df.agg({'pos': 'sum'}).collect()[0][0]\n",
    "neg_sum = count_df.agg({'neg': 'sum'}).collect()[0][0]\n",
    "count_df = count_df.withColumn('pos', col('pos') * 100 / pos_sum)\n",
    "count_df = count_df.withColumn('neg', col('neg') * 100 / neg_sum)\n",
    "count_df.sort(col('sum').desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f6eec-8dc5-46be-a9b0-ea1156efb7ea",
   "metadata": {},
   "source": [
    "These are the top 10 frequent words. However, since pos and neg values are nearly the same, the final score will be close to 0, meaning these words have limited influence on our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44433648-3bc1-46c1-a96f-32c964c72136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def clip(val, floor, ceiling):\n",
    "    if val < floor:\n",
    "        val = floor\n",
    "    elif val > ceiling:\n",
    "        val = ceiling\n",
    "    return val\n",
    "clip_udf = udf(clip, DoubleType())\n",
    "\n",
    "# We lower bound with small value to avoid having log(0). Upper bound doesn't have any meaning but we need to feed some value to the function.\n",
    "lower, upper = 1e-7, 100\n",
    "count_df = count_df.withColumn('pos', clip_udf(col('pos'), lit(lower), lit(upper)))\n",
    "count_df = count_df.withColumn('neg', clip_udf(col('neg'), lit(lower), lit(upper)))\n",
    "\n",
    "# Computing the score. Since less frequent values tend to get high score, we bound it.\n",
    "count_df = count_df.withColumn('score', log(col('pos')) - log(col('neg')))\n",
    "lower, upper = -0.5, 0.5\n",
    "count_df = count_df.withColumn('score', clip_udf(col('score'), lit(lower), lit(upper)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbe6fc-dfd6-4b53-930b-98d2582ad642",
   "metadata": {},
   "source": [
    "Below is the words with top 10 highest scores, which means they show up more in the positive contexts (Days when DJIA rose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9290ff-01c3-4944-a435-b4de42802f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+---+-------------------+\n",
      "|    word|                 pos|                 neg|sum|              score|\n",
      "+--------+--------------------+--------------------+---+-------------------+\n",
      "|chemical|0.036229696274379566|0.020080957967384298|116|                0.5|\n",
      "| spanish|0.038087629416655444|0.020609404229683884|121|                0.5|\n",
      "| doctors|  0.0357652129888106|0.021666296754283058|118|                0.5|\n",
      "| nigeria| 0.03251382998982782| 0.01955251170508471|107|                0.5|\n",
      "|   coast| 0.04226797898677616| 0.02430852806578099|137|                0.5|\n",
      "|     non| 0.03855211270222441| 0.02272318927888223|126|                0.5|\n",
      "|hospital| 0.03437176313210369| 0.02113785049198347|114|0.48617009251120136|\n",
      "|   judge| 0.04273246227234513|0.026422313114979338|142|0.48075002504186237|\n",
      "|northern| 0.04273246227234513|0.026422313114979338|142|0.48075002504186237|\n",
      "|    jews|0.039945562558931315|0.024836974328080578|133| 0.4751841479644172|\n",
      "+--------+--------------------+--------------------+---+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_df.sort(col('score').desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6214258-021c-4095-bb32-1155bd9a0cc4",
   "metadata": {},
   "source": [
    "Similarly, below is the words with top 10 lowest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e38d6e98-28ea-46eb-8fd9-95396e203c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+---+--------------------+\n",
      "|     word|                 pos|                 neg|sum|               score|\n",
      "+---------+--------------------+--------------------+---+--------------------+\n",
      "|   speech|  0.0222951977073105|0.036991238360971075|118|                -0.5|\n",
      "|      bin|0.017650364851620814| 0.03329211452487397|101|                -0.5|\n",
      "|  beijing|0.020901747850603596| 0.03487745331177273|111|                -0.5|\n",
      "|      web|0.020901747850603596|0.033820560787173555|109|-0.48123614016838445|\n",
      "|   search|0.019508297993896688|0.031178329475675618|101|  -0.468883372201383|\n",
      "|    users| 0.02415313084958638|0.038576577147869835|125|-0.46823126914599555|\n",
      "|   monday| 0.03344279656096576|0.052844626229958676|172| -0.4575196135510682|\n",
      "|     hong|0.020437264565034627| 0.03170677573797521|104|  -0.439170474882872|\n",
      "|ukrainian|0.032049346704258845|0.049673948656161156|163|-0.43820382425177673|\n",
      "|     kong|0.020437264565034627| 0.03064988321337603|102| -0.4052689232071902|\n",
      "+---------+--------------------+--------------------+---+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_df.sort(col('score')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4cd86-3fd0-470d-92ff-7c57b40bc619",
   "metadata": {},
   "source": [
    "## 3. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a4c6c4b-9665-4a5b-a564-bac182771505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each word in test data, we assign the computed score by left join.\n",
    "test_df = test_df.join(count_df.select(col('word'), col('score')), ['word'], 'left')\n",
    "test_df = test_df.na.fill(value=0, subset=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23119c0-ea8f-44d2-82bd-898b9cbf1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupBy to aggregate the word scores.\n",
    "# min('Label') is just keeping one label (ground truth) for each day. Taking minimum is not necessary since the labels are the same for a certain day.\n",
    "test_df = test_df.select(col('Date'), col('Label'), col('score')).groupBy('Date').agg(min('Label'),sum('score'))\n",
    "test_df = test_df.withColumnRenamed('min(Label)', 'Label').withColumnRenamed('sum(score)', 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2d937ad2-2a31-4c7e-a337-49dac76fe5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Finally, compute the accuracy.\n",
    "acc = test_df.where((col('Label')==1)==(col('sum')>0)).count() / test_df.count()\n",
    "print(f\"Test Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f8e75fc3-514b-4eeb-afe6-5bad9b4e3849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 60104)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/bigdata-fall22/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/envs/bigdata-fall22/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/envs/bigdata-fall22/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/envs/bigdata-fall22/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/envs/bigdata-fall22/lib/python3.7/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/envs/bigdata-fall22/lib/python3.7/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/opt/conda/envs/bigdata-fall22/lib/python3.7/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/conda/envs/bigdata-fall22/lib/python3.7/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0d6bd-f6ff-48b5-b8ec-32c58bf0cf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-fall22]",
   "language": "python",
   "name": "conda-env-bigdata-fall22-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
