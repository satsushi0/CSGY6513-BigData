{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsjkLd5hBAus"
   },
   "source": [
    "# **Machine Learning Implentation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYD1vIMR_2Wy"
   },
   "source": [
    "**Installing Spark Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wErI8lZsB29s"
   },
   "outputs": [],
   "source": [
    "pip install pyspark\n",
    "pip install bloom-filter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elxt4G7pME6u"
   },
   "source": [
    "** Import required libraries for the project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_znrd8GkLs8P"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, udf, concat_ws, concat, to_date, collect_list, translate, regexp_replace, when\n",
    "from pyspark.sql.types import BooleanType, StringType\n",
    "from bloom_filter2 import BloomFilter\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression, RandomForestClassifier, DecisionTreeClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRbVgvlnBJ89"
   },
   "source": [
    "**Instantiate a Spark Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bT8EN4LsBPwV",
    "outputId": "ecaa34b3-a7a1-4ea4-e571-dd7a3a0cb48c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/christopherwan/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/22 17:04:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/22 17:04:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/22 17:04:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SentimentAnalyzer').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlBE8YwrAAXg"
   },
   "source": [
    "**Loading Reddit Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ac-LOCYCBSUo",
    "outputId": "940e94df-8cae-4afd-b79c-421258872894"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data and rename column\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"multiLine\", \"true\")\\\n",
    "    .option(\"delimiter\", \"Â¥\")\\\n",
    "    .load(\"data/reddit-data-cleaned.csv\")\\\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0YZghblLs8S"
   },
   "source": [
    "**Preprocessing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rje0VFBIb6Dr"
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('comment', lower(col('comment')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1-wEOr9b6Ha"
   },
   "outputs": [],
   "source": [
    "# filter to see if title column contains any keyword from keywords\n",
    "keywords = [\"SP500\" , \"S&P500\"]\n",
    "def my_filter(col):\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in col.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "filterUDF = udf(my_filter, BooleanType())\n",
    "ids = df.filter(col(\"title\").isNotNull()).filter(filterUDF('title')).select(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_-J7mgUb6LP",
    "outputId": "9231132a-f6cb-44a0-dd87-5fd8569ef142"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create and populate bloom filter\n",
    "bloomFilterIDS = BloomFilter(ids.count(), 0.000000001)\n",
    "collected_ids = ids.collect()\n",
    "for row in collected_ids:\n",
    "    bloomFilterIDS.add(row[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odMUrmNPcPUE"
   },
   "outputs": [],
   "source": [
    "broadcastFilterIds = spark.sparkContext.broadcast(bloomFilterIDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVqQ0kN8cTAY"
   },
   "outputs": [],
   "source": [
    "def my_filter_by_ids(col):\n",
    "    return col in broadcastFilterIds.value\n",
    "        \n",
    "filterIdUDF = udf(my_filter_by_ids, BooleanType())\n",
    "bloomedFilteredData = df.filter(col(\"SP500\").isNotNull()).filter(filterIdUDF('ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rC8yXlYxdMP"
   },
   "outputs": [],
   "source": [
    "bloomedFilteredData = bloomedFilteredData.withColumn(\"date_stock\",to_date(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcXqIQIyzYcK"
   },
   "outputs": [],
   "source": [
    "bloomedFilteredData = bloomedFilteredData.na.drop(subset=[\"comment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiIlqF3Y7A_w"
   },
   "outputs": [],
   "source": [
    "bloomedFilteredData= bloomedFilteredData.drop(\"_c0\",\"id\",\"title\", \"timestamp\", \"time_key\", \"TESLA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9EI00oMdyfM"
   },
   "outputs": [],
   "source": [
    "df1 = bloomedFilteredData.groupby('date_stock', 'SP500').agg(collect_list('comment').alias(\"comment\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bO9UBnC7DG_I"
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"comment\",\n",
    "   concat_ws(\",\",col(\"comment\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDm_lBcgCYcH"
   },
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('comment', translate('comment', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~', '\" '))\n",
    "df2 = df2.withColumn('comment', regexp_replace('comment', '\"', ' '))\n",
    "df2 = df2.withColumn('comment', regexp_replace('comment', \"'\", ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ti1eQFlpFNka",
    "outputId": "2b85e71b-9b06-411f-fae5-dd0ccf921da0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|date_stock|SP500              |comment                                                                                                                                                                                                                                                                                           |\n",
      "+----------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2022-05-04|0.02986242108440229|hey dangmemesub please submit your post as a text post and add some additional context make sure to include the link \n",
      "\n",
      "i am a bot and this action was performed automatically please contact the moderators of this subredditmessagecomposetorwallstreetbets if you have any questions or concerns|\n",
      "+----------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(df2.date_stock == \"2022-05-04\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z62LvOV6HBdb"
   },
   "outputs": [],
   "source": [
    "df2= df2.withColumn(\"SP500\", when(df2[\"SP500\"]>0,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RUU5e5pKSGr"
   },
   "outputs": [],
   "source": [
    "df2= df2.withColumnRenamed(\"SP500\",\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miGOeAEQMTW9"
   },
   "source": [
    "**Spark ML pipeline setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2zEllvHQk0G",
    "outputId": "43ae0279-8802-4346-d6d6-5ec5f75905f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RegexTokenizer_88344dd24e65\n",
      "\n",
      " StopWordsRemover_aed35d0842b3\n",
      "\n",
      " CountVectorizer_8fb9d450a06a\n",
      "\n",
      " VectorAssembler_de7bb12b9159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = []\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "stages += [regexTokenizer]\n",
    "\n",
    "swr = StopWordsRemover(inputCol=\"tokens\", outputCol=\"Comments\")\n",
    "stages += [swr]\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"Comments\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n",
    "stages += [cv]\n",
    "\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n",
    "stages += [vecAssembler]\n",
    "\n",
    "[print('\\n', stage) for stage in stages]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQzn-A51Ls8W"
   },
   "source": [
    "**Training and testing models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwAdXy22oGYl"
   },
   "source": [
    "##Pipeline Fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEoeLrc2oSRs",
    "outputId": "a1e5e321-dadb-4a22-8885-098aeeda4a97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "data = pipeline.fit(df2).transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6egja6DLLs8W"
   },
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzg_LXEBMatD"
   },
   "source": [
    "## Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jp8SQE7LtIVt",
    "outputId": "b31a3eb2-ecdd-44fb-b21b-510882602511"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TF1hgaEs63ng",
    "outputId": "f0d60aae-87ff-4fbc-ff31-8144372c62b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:======================================>              (146 + 10) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|label|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|1    |1.0       |[2.3070935099073077E-9,0.9999999976929066]|\n",
      "|0    |1.0       |[0.41053334654497303,0.589466653455027]   |\n",
      "|0    |0.0       |[0.9997232192953159,2.7678070468406627E-4]|\n",
      "|0    |0.0       |[0.9999215716457134,7.842835428658029E-5] |\n",
      "|0    |0.0       |[0.9999992813957496,7.186042504598525E-7] |\n",
      "|0    |1.0       |[9.60944879558698E-10,0.9999999990390551] |\n",
      "|0    |1.0       |[0.41053334654497303,0.589466653455027]   |\n",
      "+-----+----------+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "# Select results to view\n",
    "predictions.limit(20).select(\"label\", \"prediction\", \"probability\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjJfLr_I7x4c",
    "outputId": "46d2bcda-e0da-4786-87f8-9ce0011fa205"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC:  0.75\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "nbaccuracy = evaluator.evaluate(predictions)\n",
    "print (\"Test Area Under ROC: \", nbaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMB5x791Mgl1"
   },
   "source": [
    "**Cross Validation Evaluation for Naive Bayes Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BO_SrTyy8O1q",
    "outputId": "2b516acd-9b0f-4f06-b09e-87feb487bdc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/22 17:20:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/12/22 17:20:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(train)\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlBBcrJE8P27",
    "outputId": "73e5f9cf-b829-4674-cd0c-b13a4fa365c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 492:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC:  0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "print (\"Test Area Under ROC: \", evaluator.evaluate(cvPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc19OnAkMoxw"
   },
   "source": [
    "## Logistic regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3J32nsys_0sh",
    "outputId": "bf1cd845-b07c-44ee-e79d-44b990d6f896"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "model2 = log_reg.fit(train)\n",
    "predictions = model2.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName('areaUnderROC')\n",
    "lgaccuracy = evaluator.evaluate(predictions)\n",
    "print(lgaccuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKZ52e7DMu3K"
   },
   "source": [
    "**Cross Validation Evaluation for logistic Rergression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hb88QAexc0G",
    "outputId": "bf8cbccc-120a-4200-d9db-c29a8b9cbc30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=log_reg, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(train)\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvWl7GAJM3fT"
   },
   "source": [
    "## Random Forest Classifier Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsrsrYTuDch7",
    "outputId": "29f3c1d5-cb39-44e0-bed7-a5987e1765c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/22 17:25:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 22 (= number of training instances)\n",
      "[Stage 1972:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier().setLabelCol('label').setFeaturesCol('features').setNumTrees(10)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName(\"areaUnderROC\")\n",
    "rfaccuracy = evaluator.evaluate(predictions)\n",
    "print(rfaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qEwUCfYM9yp"
   },
   "source": [
    "**Cross Validation Evaluation for Randon Forest Classifier Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loPyo-4pxi57",
    "outputId": "e0ffb0bc-5498-428e-bd9f-3c1c026803db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/22 17:25:25 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:25:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:25:42 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:25:46 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:25:51 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:25:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:25:59 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:26:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:26:15 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:26:28 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:26:33 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:26:38 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:26:43 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:26:47 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:26:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:26:56 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:27:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:27:21 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:27:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:27:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:27:35 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:27:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:27:44 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:27:49 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:28:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 22 (= number of training instances)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5833333333333333"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(train)\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xBL_nh1NFYr"
   },
   "source": [
    "## Decision Tree Classifier Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c-BNl8ToHBW",
    "outputId": "4ab35c96-72f5-467c-bbaf-c4b50b8afaac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/22 17:28:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 22 (= number of training instances)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setRawPredictionCol('prediction')\n",
    "#evaluator = BinaryClassificationEvaluator(labelCol=\"label\", featuresCol=\"features\", maxDepth=2)\n",
    "dtAccuracy = evaluator.evaluate(predictions)\n",
    "print(dtAccuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM8CPOcINJqh"
   },
   "source": [
    "**Cross Validation Evaluation for Decision Tree Clasifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grX1vvG_xlT8",
    "outputId": "f50b387f-753e-43a9-a9c9-aec3e40f03da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/22 17:29:00 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:29:13 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:29:18 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:29:22 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:29:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:29:31 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:29:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:29:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "22/12/22 17:29:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:30:01 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:30:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:30:06 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:30:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:30:11 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:30:14 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:30:17 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
      "22/12/22 17:30:27 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:30:38 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:30:40 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:30:43 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:30:45 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:30:48 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:30:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:30:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
      "22/12/22 17:31:11 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 22 (= number of training instances)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(train)\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0fTSyE-NR2y"
   },
   "source": [
    "## Support Vector Classifier Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hzbUUxWyJwN",
    "outputId": "4ddbd5bf-4dad-4c51-9516-172869af9c7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4252:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define your classifier\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(train)\n",
    "\n",
    "# Compute predictions for test data\n",
    "predictions = lsvcModel.transform(test)\n",
    "\n",
    "# Define the evaluator method with the corresponding metric and compute the classification error on test data\n",
    "evaluator = BinaryClassificationEvaluator().setRawPredictionCol('prediction')\n",
    "svmaccuracy = evaluator.evaluate(predictions) \n",
    "\n",
    "# Show the accuracy\n",
    "print(\"Test accuracy = %g\" % (svmaccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8GY5s6UNWT4"
   },
   "source": [
    "**Cross Validation Evaluation of Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_FoTToNxpud",
    "outputId": "1211b28c-968b-4136-b5e2-97d51059f1b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=lsvc, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(train)\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
